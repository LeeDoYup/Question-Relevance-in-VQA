%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn
\usepackage{booktabs} % For formal tables
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}

\def\UrlBreaks{\do\/\do-}
% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
% \setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

%Conference
\acmConference[]{}
\copyrightyear{}

\acmArticle{4}
\acmPrice{15.00}

\begin{document}
\title{Question Relevance in Visual Question Answering}
\author{Prakruthi Prabhakar}
\affiliation{%
  \institution{ML with Large Datasets (10-805) \\ Carnegie Mellon University}
  \city{Pittsburgh} 
  \state{PA} 
  \postcode{15217}
}
\email{prakrutp@andrew.cmu.edu}

\author{Nitish Kulkarni}
\affiliation{%
  \institution{ML with Large Datasets (10-805) \\ Carnegie Mellon University}
  \city{Pittsburgh}
  \state{PA}
  \postcode{15217}
}
\email{nitishkk@andrew.cmu.edu}

\author{Linghao Zhang}
\affiliation{%
  \institution{ML with Large Datasets (10-805) \\ Carnegie Mellon University}
  \city{Pittsburgh} 
  \state{PA} 
  \postcode{15217}
}
\email{linghaoz@andrew.cmu.edu}

\begin{abstract}
Free-form and open-ended Visual Question Answering systems solve the problem of providing an accurate natural language answer to a question pertaining to an image. Current VQA systems do not evaluate if the posed question is relevant to the input image and hence provide nonsensical answers when posed with irrelevant questions to an image. In this paper, we solve the problem of identifying the relevance of the posed question to an image. We address the problem as two sub-problems. We first identify if the question is visual or not. If the question is visual, we then determine if it's relevant to the image or not. 
For the second problem, we generate a large dataset from existing visual question answering datasets in order to enable the training of complex architectures and model the relevance of a visual question to an image. We also compare the results of our Long Short-Term Memory Recurrent Neural Network based models to Logistic Regression, XGBoost and multi-layer perceptron based approaches to the problem.
\end{abstract}

\maketitle

\input{body}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography} 

\end{document}
